{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2888b630",
   "metadata": {},
   "source": [
    "## Loading Libraries & Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac73e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "import re\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Scipy & Statsmodels\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, boxcox, mstats, randint\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LinearRegression, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, root_mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, plot_importance\n",
    "\n",
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Scikit-Optimize\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce549770",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = pd.read_csv(r\"laptop_prices.csv\")\n",
    "path_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f017b",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6074045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_transformers import (\n",
    "    CPUSeriesExtractor,\n",
    "    GPUSeriesExtractor,\n",
    "    CardinalityReducer,\n",
    "    PixelCalculator,\n",
    "    LogTransformer,\n",
    "    ColumnDropper,\n",
    "    KMeansClusterAdder\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e61e5d0",
   "metadata": {},
   "source": [
    "## A quick overview **(EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7197d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d8e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e5e5d",
   "metadata": {},
   "source": [
    "No missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d54a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(path_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728fd18",
   "metadata": {},
   "source": [
    "No duplicates in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d550d0",
   "metadata": {},
   "source": [
    "### ~`Company`\n",
    "Column seems to have a high cardinality between it's results due to companies having bigger market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40287559",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = path_df[\"Company\"].unique()\n",
    "print(f\"Laptop companies: {company_names}\")\n",
    "\n",
    "plt.figure(figsize= (18,12))\n",
    "\n",
    "company_counts = path_df[\"Company\"].value_counts()\n",
    "avg_company_count = company_counts.mean()\n",
    "\n",
    "# List comprehension for color condition\n",
    "colors = [\"green\" if count >= avg_company_count else \"red\" for count in company_counts]\n",
    "\n",
    "sns.countplot(x = \"Company\",\n",
    "              data = path_df,\n",
    "              order = company_counts.index,\n",
    "              palette = colors,\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Market shares of laptop companies\")\n",
    "# Average count line\n",
    "plt.axhline(y = avg_company_count,\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label = f\"Mean: {avg_company_count:.0f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416bd03",
   "metadata": {},
   "source": [
    "### `Product`\n",
    "Column has repeated entries (nothing concerning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5bb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df[\"Product\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfaab63",
   "metadata": {},
   "source": [
    "### ~`TypeName`\n",
    "**Notebooks** seem to be the most popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98e53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_types = path_df[\"TypeName\"].nunique()\n",
    "print(f\"Laptop types: {unique_types}\")\n",
    "\n",
    "plt.figure(figsize = (18,12))\n",
    "\n",
    "typecounts = path_df[\"TypeName\"].value_counts()\n",
    "\n",
    "sns.countplot(x = \"TypeName\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\",\n",
    "              order = typecounts.index)\n",
    "plt.title(\"Laptop type popularity\")\n",
    "plt.xlabel(\"Laptop Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67aa9f3",
   "metadata": {},
   "source": [
    "### `Inches`\n",
    "Column seemed to have multiple high leverage points that after a deep research it was concluded that they weren't outliers. They were small sized due to them mostly being convertibles or small netbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7724b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_size = path_df[\"Inches\"].mean()\n",
    "print(f\"Average laptop size: {avg_size:.0f} inches\")\n",
    "\n",
    "Q1 = path_df[\"Inches\"].quantile(0.25)\n",
    "Q3 = path_df[\"Inches\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = path_df[(path_df[\"Inches\"] < lower) | (path_df[\"Inches\"] > upper)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "display(outliers)\n",
    "\n",
    "plt.figure(figsize = (18,12))\n",
    "sns.boxplot(x = \"Inches\",\n",
    "            data = path_df,\n",
    "            color = \"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b8b56",
   "metadata": {},
   "source": [
    "### `Ram`\n",
    "Just like last column. `Ram` Also had high leverage points in terms of ram capacity beihng above the average in some cases. That was due to these laptops being a high performance **gaming** or **workstation** laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f9a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_ram = path_df[\"Ram\"].mean()\n",
    "print(f\"Average laptop ram: {avg_ram:.0f} GB\")\n",
    "\n",
    "Q1 = path_df[\"Ram\"].quantile(0.25)\n",
    "Q3 = path_df[\"Ram\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = path_df[(path_df[\"Ram\"] < lower) | (path_df[\"Ram\"] > upper)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "display(outliers)\n",
    "\n",
    "plt.figure(figsize = (18,12))\n",
    "sns.boxplot(x = \"Ram\",\n",
    "            data = path_df,\n",
    "            color = \"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef8ca7",
   "metadata": {},
   "source": [
    "### ~`OS`\n",
    "We see high cardinality of categorical variables with a large dominance of windows 10 being the main OS for most laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a71d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_counts = path_df[\"OS\"].value_counts()\n",
    "os_avg = os_counts.mean()\n",
    "os_num = path_df[\"OS\"].nunique()\n",
    "print(f\"There are a total of {os_num} operating systems in the market\")\n",
    "\n",
    "color = [\"green\" if count > os_avg else \"red\" for count in os_counts]\n",
    "\n",
    "plt.figure(figsize = (18,12))\n",
    "sns.countplot(x = \"OS\",\n",
    "              data = path_df,\n",
    "              palette = color,\n",
    "              order = os_counts.index,\n",
    "              edgecolor = \"black\")\n",
    "plt.axhline(y = os_avg,\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label = f\"Mean: {os_avg:.0f}\")\n",
    "plt.legend()\n",
    "plt.title(\"Preinstalled operating systems in laptops\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075faf3c",
   "metadata": {},
   "source": [
    "### `Weight`\n",
    "There seems to be a couple influencial entires where they weigh more than 2.71 KG but it's to be expected due to their bigger surface area compared to other laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_weight = path_df[\"Weight\"].mean()\n",
    "print(f\"Average weight: {avg_weight:.2f} KG\")\n",
    "\n",
    "Q1 = path_df[\"Weight\"].quantile(0.25)\n",
    "Q3 = path_df[\"Weight\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = path_df[(path_df[\"Weight\"] < lower) | (path_df[\"Weight\"] > upper)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "display(outliers)\n",
    "\n",
    "plt.figure(figsize = (18,12))\n",
    "sns.boxplot(x = \"Weight\",\n",
    "            data = path_df,\n",
    "            color = \"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7709ca",
   "metadata": {},
   "source": [
    "As we see, there's a positive linear relationship between the laptop size in inches and its weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4beaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.regplot(x = \"Inches\",\n",
    "                y = \"Weight\",\n",
    "                data = path_df,\n",
    "                scatter_kws = {\"color\" : \"grey\", \"alpha\" : 0.8},\n",
    "                line_kws = {\"color\" : \"red\", \"linewidth\" : 2})\n",
    "plt.title(\"Relationship between laptop size & weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f8ab1",
   "metadata": {},
   "source": [
    "### `Price_euros`\n",
    "Price seems to be skewed which will need transformation to adjust and normalize or checking outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_skew(x):\n",
    "    if abs(x) > 1:\n",
    "        return \"Very skewed\"\n",
    "    elif 0.5 <= abs(x) <= 1:\n",
    "        return \"Moderately skewed\"\n",
    "    else : \n",
    "        return \"Symmetrical\"\n",
    "def state_kurtosis(x):\n",
    "    if x > 1:\n",
    "        return \"Leptokurtic (Many Outliers)\"\n",
    "    elif x < -1:\n",
    "        return \"Playtukurtic (Few Outliers)\"\n",
    "    else :\n",
    "        return \"Mesokurtic (Normal tails)\"\n",
    "    \n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))\n",
    "sns.histplot(x = \"Price_euros\",\n",
    "             data = path_df,\n",
    "             color = \"grey\",\n",
    "             edgecolor = \"black\",\n",
    "             kde = True,\n",
    "             ax = ax1)\n",
    "ax1.set_title(\"Checking column normality\")\n",
    "sns.regplot(x = \"Price_euros\",\n",
    "                y = \"Weight\",\n",
    "                data = path_df,\n",
    "                scatter_kws = {\"color\" : \"grey\", \"alpha\" : 0.8},\n",
    "                line_kws = {\"color\" : \"red\", \"linewidth\" : 2},\n",
    "                ax = ax2)\n",
    "ax2.set_title(\"Relationship of price with weight\")\n",
    "plt.show()\n",
    "\n",
    "price_kurtosis = path_df[\"Price_euros\"].kurtosis()\n",
    "price_skew = path_df[\"Price_euros\"].skew()\n",
    "print(f\"Price kurtosis before transformation: {price_kurtosis:.2f}, {state_kurtosis(price_kurtosis)}\")\n",
    "print(f\"Price skew before transformation: {price_skew:.2f}, {state_skew(price_skew)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a02a47",
   "metadata": {},
   "source": [
    "From what we can see is there are multiple outliers that could affect the modeling phase.\n",
    "After further investigation they could seem dangerous but after transformation most likely it'll be completely normalized due to these highly priced laptops being either workstation, ultrabooks or gaming laptops that have alot of expensive parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb2fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_price = path_df[\"Price_euros\"].mean()\n",
    "print(f\"Average price: {avg_price:.2f}€\")\n",
    "\n",
    "Q1 = path_df[\"Price_euros\"].quantile(0.25)\n",
    "Q3 = path_df[\"Price_euros\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q1 + 1.5 * IQR\n",
    "\n",
    "outliers = path_df[(path_df[\"Price_euros\"] < lower) | (path_df[\"Price_euros\"] > upper)]\n",
    "print(f\"Number of outliers: {len(outliers)}\")\n",
    "print(f\"Outlier bounds: [{lower:.2f}, {upper:.2f}]\")\n",
    "display(outliers)\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.boxplot(x = \"Price_euros\",\n",
    "            data = path_df,\n",
    "            color = \"grey\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45995d",
   "metadata": {},
   "source": [
    "### ~`Screen`, `ScreenW`, & `ScreemH`\n",
    "Dominance of **Full HD** & **Standard** over the laptop market (High Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be67a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_count = path_df[\"Screen\"].value_counts()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "plt.pie(screen_count.values,\n",
    "        colors = sns.color_palette(\"Set2\"),\n",
    "        autopct = \"%1.1f%%\",\n",
    "        explode = [0, 0.1, 0, 0])  # Shows percentage\n",
    "plt.title(\"Laptop screens by popularity\")\n",
    "plt.legend(labels = screen_count.index,\n",
    "           title = \"Screen type\",\n",
    "           loc = \"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc741915",
   "metadata": {},
   "source": [
    "**1920x1080p** dominated laptop screens and to this day it's the most popular and most used resolution although **2560x1440p** & **3840x2160p** (also known as **4K**) has been getting more popular in the recent years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765611f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the full screen resolution\n",
    "path_df[\"Resolution\"] = path_df[\"ScreenW\"].astype(str) + \"x\" + path_df[\"ScreenH\"].astype(str)\n",
    "res_count = path_df[\"Resolution\"].value_counts()\n",
    "avg_res = res_count.mean()\n",
    "\n",
    "color = [\"green\" if count >= avg_res else \"red\" for count in res_count]\n",
    "\n",
    "plt.figure(figsize = (18, 12))\n",
    "sns.countplot(x = \"Resolution\",\n",
    "              data = path_df,\n",
    "              order = res_count.index,\n",
    "              palette = color,\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Most popular laptop resolutions\")\n",
    "plt.axhline(y = avg_res,\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label = f\"Mean: {avg_res:.0f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f9f69",
   "metadata": {},
   "source": [
    "### ~`Touchscreen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96d0476",
   "metadata": {},
   "outputs": [],
   "source": [
    "touch_count = path_df[\"Touchscreen\"].value_counts()\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.pie(touch_count.values,\n",
    "        colors = sns.color_palette(\"Set2\"),\n",
    "        autopct = \"%1.1f%%\",\n",
    "        explode = [0, 0.1])\n",
    "plt.title(\"Touchscreen laptop proportion\")\n",
    "plt.legend(labels = touch_count.index,\n",
    "           title = \"Touchscreen ?\",\n",
    "           loc = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332462c3",
   "metadata": {},
   "source": [
    "### ~`IPSpanel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8025fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ips_count = path_df[\"IPSpanel\"].value_counts()\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "plt.pie(ips_count.values,\n",
    "        colors = sns.color_palette(\"Set2\"),\n",
    "        autopct = \"%1.1f%%\",\n",
    "        explode = [0, 0.1])\n",
    "plt.title(\"IPS screens laptop proportion\")\n",
    "plt.legend(labels = ips_count.index,\n",
    "           title = \"IPS ?\",\n",
    "           loc = \"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13ef25",
   "metadata": {},
   "source": [
    "### ~`RetinaDisplay`\n",
    "Very little laptops had retina display screens where this technology was mostly unique to apple macbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c62022",
   "metadata": {},
   "outputs": [],
   "source": [
    "retina_count = path_df[\"RetinaDisplay\"].value_counts()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (10,8))\n",
    "ax2.pie(retina_count.values,\n",
    "        colors = sns.color_palette(\"Set2\"),\n",
    "        autopct = \"%1.1f%%\")\n",
    "ax2.legend(labels = retina_count.index,\n",
    "           title = \"Retina ?\",\n",
    "           loc = \"upper right\")\n",
    "ax2.set_title(\"Retina display laptop proportion\")\n",
    "\n",
    "# Showing only companies making retina display laptops\n",
    "retina_by_company = path_df[path_df[\"RetinaDisplay\"] == \"Yes\"].groupby(\"Company\")[\"RetinaDisplay\"].count().sort_values(ascending = False)\n",
    "retina_companies = retina_by_company[retina_by_company > 0]\n",
    "\n",
    "sns.barplot(x = retina_companies.index,\n",
    "            y = retina_companies.values,\n",
    "            color = \"grey\",\n",
    "            edgecolor = \"black\",\n",
    "            ax = ax1)\n",
    "ax1.set_title(\"Retina display by company\")\n",
    "ax1.tick_params(axis = \"x\", rotation = 45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Only {retina_companies.shape[0]} companies have retina display laptops\")\n",
    "print(f\"Total Retina laptops: {retina_count.get(1, 0)} ({retina_count.get(1, 0)/len(path_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487239c",
   "metadata": {},
   "source": [
    "### ~`CPU_company`\n",
    "Intel holds a massive market share in CPUs for laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164b590",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 10))\n",
    "sns.countplot(x = \"CPU_company\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Laptop cpu companies market shares\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d29893",
   "metadata": {},
   "source": [
    "### `CPU_freq`\n",
    "Although **Intel** holds the bigger marketshare. **AMD** seems to have the better performing CPUs on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1547e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_company_freq = path_df.groupby(\"CPU_company\")[\"CPU_freq\"].mean().sort_values(ascending = False).round(1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12, 10))\n",
    "sns.histplot(x = \"CPU_freq\",\n",
    "             data = path_df,\n",
    "             color = \"grey\",\n",
    "             edgecolor = \"black\",\n",
    "             kde = True,\n",
    "             ax = ax1)\n",
    "ax1.set_title(\"Laptop CPU clock speed distribution\")\n",
    "ax1.set_ylabel(\"Frequency (GHz)\")\n",
    "\n",
    "sns.barplot(x = cpu_company_freq.index,\n",
    "            y = cpu_company_freq.values,\n",
    "            palette = \"Set2\",\n",
    "            edgecolor = \"black\",\n",
    "            ax = ax2)\n",
    "ax2.set_title(\"Average CPU clock speed per company\")\n",
    "ax2.set_ylabel(\"Frequency (GHz)\")\n",
    "for i, v in enumerate(cpu_company_freq.values):\n",
    "    ax2.text(i, v - 0.15, f\"{v:.1f}GHz\", ha = \"center\", fontweight = \"bold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac214af",
   "metadata": {},
   "source": [
    "### ~`CPU_model`\n",
    "There is a massive popularity of \"**Core i**\" models compared to other models of **Intel** and other CPU companies (High Cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23550713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting \n",
    "def extract_cpu_model(cpu_string):\n",
    "    patterns = [\n",
    "        r'(Core [im]\\d)',           # Core i3, i5, i7, i9, m3, m5, etc...\n",
    "        r'(Ryzen \\d)',              # Ryzen 3, 5, 7\n",
    "        r'(Atom [xX]\\d)',           # Atom x5, x7\n",
    "        r'(Celeron)',               # Celeron\n",
    "        r'(Pentium)',               # Pentium\n",
    "        r'(Xeon)',                  # Xeon\n",
    "        r'(A\\d+)',                  # AMD A6, A9, A10, A12, etc...\n",
    "        r'(E2)',                    # AMD E2\n",
    "        r'(FX)',                    # AMD FX\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, cpu_string, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return \"Other\"\n",
    "\n",
    "path_df[\"CPU_series\"] = path_df[\"CPU_model\"].apply(extract_cpu_model)\n",
    "series_counts = path_df[\"CPU_series\"].value_counts()\n",
    "avg_series = series_counts.mean()\n",
    "\n",
    "colors = [\"green\" if count > avg_series else \"red\" for count in series_counts]\n",
    "\n",
    "plt.figure(figsize = (16,10))\n",
    "sns.countplot(x = \"CPU_series\",\n",
    "              data = path_df,\n",
    "              palette = colors,\n",
    "              order = series_counts.index,\n",
    "              edgecolor = \"black\")\n",
    "plt.axhline(y = avg_series,\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label =f\"Mean: {avg_series:.0f}\")\n",
    "plt.legend()\n",
    "plt.title(\"CPU models popularity in laptops\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cf3d36",
   "metadata": {},
   "source": [
    "### `PrimaryStorage`\n",
    "Most laptops had **256GBs** as a primary drive that had their operating system on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb9dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(x = \"PrimaryStorage\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Primary storage popularity in GBs\")\n",
    "plt.xlabel(\"GBs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a3a82",
   "metadata": {},
   "source": [
    "### `SecondaryStorage`\n",
    "Most laptops don't have any secondary hard drives installed which could result in an imbalance in modeling later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e1f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(x = \"SecondaryStorage\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Secondary storage popularity in GBs\")\n",
    "plt.xlabel(\"GBs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba22a77",
   "metadata": {},
   "source": [
    "### ~`PrimaryStorageType`\n",
    "**SSDs** generally have been the most popular storage type due to their fast read and write frequency & longevity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e356e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(x = \"PrimaryStorageType\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Primary storage type popularity\")\n",
    "plt.xlabel(\"Storage Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4071fd",
   "metadata": {},
   "source": [
    "### ~`SecondaryStorageType`\n",
    "Surprisingly laptops that do have a seconadry storage installed usually use **HDD** which means that usually use secondary storage to store important files and don't need to use the massive read & write speeds that an **SSD** provides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ea77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(x = \"SecondaryStorageType\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Secondary storage type popularity\")\n",
    "plt.xlabel(\"Storage Type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e940605",
   "metadata": {},
   "source": [
    "### ~`GPU_company`\n",
    "**Intel** has the largest market share which is expected due to them winning by a landslide in CPUs as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960cbbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "sns.countplot(x = \"GPU_company\",\n",
    "              data = path_df,\n",
    "              palette = \"Set2\",\n",
    "              edgecolor = \"black\")\n",
    "plt.title(\"Laptop GPU companies market share\")\n",
    "plt.xlabel(\"GPU Companies\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3194086f",
   "metadata": {},
   "source": [
    "### ~`GPU_model`\n",
    "**Intel integrated graphics** had the highest market share due to them being present in most budget to medium priced laptop without a dedicated GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting GPU series\n",
    "def extract_gpu_model(gpu_string):\n",
    "    gpu_string = str(gpu_string)\n",
    "    \n",
    "    # NVIDIA patterns\n",
    "    if re.search(r'GTX', gpu_string, re.IGNORECASE):                    # GTX\n",
    "        return \"GeForce GTX\"\n",
    "    elif re.search(r'MX\\d{3}', gpu_string, re.IGNORECASE):              # MX\n",
    "        return \"GeForce MX\"\n",
    "    elif re.search(r'GeForce \\d{3}', gpu_string, re.IGNORECASE):        # GeForce\n",
    "        return \"GeForce (Low-end)\"\n",
    "    elif re.search(r'Quadro', gpu_string, re.IGNORECASE):               # Quadro\n",
    "        return \"Quadro\"\n",
    "    \n",
    "    # AMD patterns\n",
    "    elif re.search(r'Radeon Pro', gpu_string, re.IGNORECASE):           # Radeon Pro\n",
    "        return \"Radeon Pro\"\n",
    "    elif re.search(r'Radeon RX', gpu_string, re.IGNORECASE):            # Radeon RX\n",
    "        return \"Radeon RX\"\n",
    "    elif re.search(r'Radeon R\\d', gpu_string, re.IGNORECASE):           # Radeon R\n",
    "        return \"Radeon R\"\n",
    "    elif re.search(r'Radeon', gpu_string, re.IGNORECASE):               # Radeon 530, etc...\n",
    "        return \"Radeon (Other)\"\n",
    "    \n",
    "    # Intel patterns\n",
    "    elif re.search(r'UHD Graphics', gpu_string, re.IGNORECASE):         # UHD Graphics 550, etc...\n",
    "        return \"Intel UHD\"\n",
    "    elif re.search(r'Iris Plus', gpu_string, re.IGNORECASE):            # Iris\n",
    "        return \"Intel Iris Plus\"\n",
    "    elif re.search(r'Iris Pro', gpu_string, re.IGNORECASE):             # Iris Pro\n",
    "        return \"Intel Iris Pro\"\n",
    "    elif re.search(r'Iris', gpu_string, re.IGNORECASE):                 # Iris\n",
    "        return \"Intel Iris\"\n",
    "    elif re.search(r'HD Graphics', gpu_string, re.IGNORECASE):          # HD Graphics\n",
    "        return \"Intel HD\"\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "path_df[\"GPU_series\"] = path_df[\"GPU_model\"].apply(extract_gpu_model)\n",
    "gpu_series_counts = path_df[\"GPU_series\"].value_counts()\n",
    "avg_gpu_series = gpu_series_counts.mean()\n",
    "\n",
    "colors = [\"green\" if count > avg_gpu_series else \"red\" for count in gpu_series_counts]\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.countplot(x = \"GPU_series\",\n",
    "              data = path_df,\n",
    "              palette = colors,\n",
    "              order = gpu_series_counts.index,\n",
    "              edgecolor = \"black\")\n",
    "plt.axhline(y = avg_gpu_series,\n",
    "            color = \"black\",\n",
    "            linestyle = \"--\",\n",
    "            label = f\"Mean: {avg_gpu_series:.0f}\")\n",
    "plt.legend()\n",
    "plt.title(\"GPU series popularity in laptops\")\n",
    "plt.xlabel(\"GPU Series\")\n",
    "plt.xticks(rotation = 45, ha = \"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00240b94",
   "metadata": {},
   "source": [
    "### Correlation Matrix (Before Transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = path_df.select_dtypes(\"number\")\n",
    "plt.figure(figsize = (12,10))\n",
    "sns.heatmap(num_df.corr(),\n",
    "            annot = True,\n",
    "            cmap = \"Greens\",\n",
    "            fmt = \".2f\",\n",
    "            linewidths = 0.2)\n",
    "plt.title(\"Correlation Matrix (Before)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251c303",
   "metadata": {},
   "source": [
    "## Choosing the variables (Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a609aa1d",
   "metadata": {},
   "source": [
    "### Pipeline configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5efadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations & encoding groups\n",
    "te_cols = [\"Company\", \"CPU_series\", \"GPU_series\"]\n",
    "\n",
    "ohe_cols = ['TypeName', 'OS', 'Screen', 'CPU_company', \n",
    "            'PrimaryStorageType', 'SecondaryStorageType', 'GPU_company']\n",
    "\n",
    "binary_cols = [\"Touchscreen\", \"IPSpanel\", \"RetinaDisplay\"]\n",
    "\n",
    "log_cols = [\"PrimaryStorage\", \"SecondaryStorage\"]\n",
    "\n",
    "# Columns getting dropped\n",
    "drop_cols = ['Product', 'CPU_model', 'GPU_model', 'ScreenW', 'ScreenH', 'Resolution']\n",
    "\n",
    "# Scaling groups\n",
    "robust_features = ['Ram', 'PrimaryStorage', 'SecondaryStorage']\n",
    "standard_features = ['Inches', 'CPU_freq', 'Pixels', \"Weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc20eb",
   "metadata": {},
   "source": [
    "### Pipeline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbf2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline with any regressor, auto log1p/expm1 on price.\n",
    "def build_pipeline(model, n_clusters = 4):\n",
    "\n",
    "    return Pipeline([\n",
    "        # Feature engineering\n",
    "        (\"cpu_extract\", CPUSeriesExtractor()),\n",
    "        (\"gpu_extract\", GPUSeriesExtractor()),\n",
    "        (\"pixels\", PixelCalculator(log_transform=True)),\n",
    "        (\"log_storage\", LogTransformer(columns=log_cols)),\n",
    "        (\"cardinality\", CardinalityReducer(\n",
    "            columns=te_cols,\n",
    "            threshold=\"mean\",\n",
    "            exceptions={\"Company\": [\"Apple\"]}\n",
    "        )),\n",
    "        (\"drop\", ColumnDropper(columns = drop_cols)),\n",
    "        \n",
    "        # Encoding & scaling\n",
    "        (\"encoding_scaling\", ColumnTransformer(transformers =[\n",
    "            (\"te\", TargetEncoder(cv = 5, smooth= \"auto\"), te_cols), # Has built in auto smoothing \n",
    "            (\"ohe\", OneHotEncoder(handle_unknown = \"ignore\", sparse_output = False, drop = \"first\"), ohe_cols),\n",
    "            (\"binary\", OrdinalEncoder(), binary_cols),\n",
    "            (\"robust\", RobustScaler(), robust_features),\n",
    "            (\"std\", StandardScaler(), standard_features)\n",
    "        ], remainder = \"passthrough\")),\n",
    "        \n",
    "        # Clustering\n",
    "        (\"cluster\", KMeansClusterAdder(n_clusters = n_clusters)),\n",
    "        \n",
    "        # Log transformations\n",
    "        (\"model\", TransformedTargetRegressor(\n",
    "            regressor = model,\n",
    "            func = np.log1p,\n",
    "            inverse_func = np.expm1\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "# Complete model hyperparameter tuning, training, & testing, outputing metrics\n",
    "def tune_and_evaluate(name, model, param_config, X_train, X_test, y_train, y_test):\n",
    "    pipe = build_pipeline(model)\n",
    "    \n",
    "    # Filters parameter labeling from pipeline\n",
    "    prefixed_params = {\n",
    "        f\"model__regressor__{k}\": v for k, v in param_config[\"params\"].items()\n",
    "    }\n",
    "    \n",
    "    if not prefixed_params:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        best_pipe = pipe\n",
    "        best_params_clean = {}\n",
    "        \n",
    "    elif param_config[\"method\"] == \"random\": # RandomSearchCV()\n",
    "        rand_search = RandomizedSearchCV(\n",
    "            estimator = pipe,\n",
    "            param_distributions = prefixed_params,\n",
    "            n_iter = param_config.get(\"n_iter\", 100),\n",
    "            cv = 10,\n",
    "            scoring = \"neg_mean_absolute_error\",\n",
    "            n_jobs = -1,\n",
    "            verbose = 1,\n",
    "            random_state = 90\n",
    "        )\n",
    "        rand_search.fit(X_train, y_train)\n",
    "        best_pipe = rand_search.best_estimator_\n",
    "        best_params_clean = {k.replace(\"model__regressor__\", \"\"): v for k, v in rand_search.best_params_.items()}\n",
    "        \n",
    "    elif param_config[\"method\"] == \"grid\": # GridSearchCV()\n",
    "        grid_search = GridSearchCV( \n",
    "            estimator = pipe,\n",
    "            param_grid = prefixed_params,\n",
    "            cv = 10,\n",
    "            scoring = \"neg_mean_absolute_error\",\n",
    "            n_jobs = -1,\n",
    "            verbose = 1\n",
    "        )\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        best_pipe = grid_search.best_estimator_\n",
    "        best_params_clean = {k.replace(\"model__regressor__\", \"\"): v for k, v in grid_search.best_params_.items()}\n",
    "        \n",
    "    else:\n",
    "        bay_search = BayesSearchCV(\n",
    "            estimator = pipe,\n",
    "            search_spaces = prefixed_params,\n",
    "            cv = 10,\n",
    "            scoring = \"neg_mean_absolute_error\",\n",
    "            n_jobs = -1,\n",
    "            verbose = 0,\n",
    "            random_state = 72\n",
    "        )\n",
    "        bay_search.fit(X_train, y_train)\n",
    "        best_pipe = bay_search.best_estimator_\n",
    "        best_params_clean = {k.replace(\"model__regressor__\", \"\"): v for k, v in bay_search.best_params_.items()}\n",
    "        \n",
    "    y_pred = best_pipe.predict(X_test)\n",
    "    \n",
    "    results = {\n",
    "        \"Model\": name,\n",
    "        \"  R2\": r2_score(y_test, y_pred),\n",
    "        \" MAE\": mean_absolute_error(y_test, y_pred),\n",
    "        \"RMSE\": root_mean_squared_error(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n===={name} results====\\n\")\n",
    "    if best_params_clean:\n",
    "        print(f\"    Method: {param_config['method'].upper()}\")\n",
    "        print(f\"    Best parameters: {best_params_clean}\")\n",
    "    for metric, value in results.items():\n",
    "        if metric != \"Model\":\n",
    "            print(f\"    {metric}: {value:.2f}\")\n",
    "    print(\"\\n--------------------------\\n\")\n",
    "    return results, best_pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295703e7",
   "metadata": {},
   "source": [
    "## Setting up data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6fc588",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = pd.read_csv(r\"laptop_prices.csv\")\n",
    "\n",
    "X = path_df.drop(columns = \"Price_euros\")\n",
    "y = path_df[\"Price_euros\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 50, test_size = 0.2)\n",
    "\n",
    "print(f\"Training on {X_train.shape[0]} laptops.\")\n",
    "print(f\"Testing on {X_test.shape[0]} laptops.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4acacf7",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "XGBoost model wins by a significant margin compared to other models in terms of **R2**, **MAE**, & **RMSE** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ece7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearRegression()\n",
    "lr_results, lr_model_pipe = tune_and_evaluate(\n",
    "    \"Linear Regression\", LinearRegression(),\n",
    "    {\"method\": None, \"params\": {}}, # LR is non hyperparametic\n",
    "    X_train, X_test, y_train, y_test    \n",
    ")\n",
    "\n",
    "# ElasticNetCV()\n",
    "en_results, en_model_pipe = tune_and_evaluate(\n",
    "    \"Elastic Net\", ElasticNetCV(l1_ratio=[0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
    "                                cv = 5,\n",
    "                                random_state = 5,\n",
    "                                alphas = np.logspace(-4, 1, 100),  # force search over smaller alphas\n",
    "                                max_iter = 10000),\n",
    "    {\"method\": None, \"params\": {}},\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "\n",
    "# RandomForestRegressor()\n",
    "rfr_results, rfr_model_pipe = tune_and_evaluate(\n",
    "    \"Random Forest Regressor\", RandomForestRegressor(random_state = 42),\n",
    "    {\n",
    "        \"method\": \"random\",\n",
    "        \"n_iter\": 100,\n",
    "        \"params\": { # Model Parameters\n",
    "            \"n_estimators\": randint(100, 500), # number of trees\n",
    "            \"max_depth\": [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, None], # maximum number of levels\n",
    "            \"min_samples_split\": randint(2, 10), # minimum number of samples required to split a node\n",
    "            \"max_features\": [\"sqrt\", \"log2\", 1.0] # number of features to consider on every split\n",
    "        }\n",
    "    },\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "xgb_results, xgb_model_pipe = tune_and_evaluate(\n",
    "    \"XGBoost\", XGBRegressor(\n",
    "        objective = \"reg:squarederror\", # focusing on minimizing squared error\n",
    "        tree_method = \"hist\", # histogram binning trees for higher speeds\n",
    "        random_state = 67,\n",
    "        n_jobs = -1 # full powaaaaaaaaa\n",
    "    ),\n",
    "    {\"method\": \"grid\",\n",
    "     \"n_iter\": 100,\n",
    "     \"params\": {\n",
    "        \"n_estimators\": [1000, 1100, 1150, 1200],\n",
    "        \"max_depth\": [5, 6, 7],\n",
    "        \"eta\": [0.01, 0.02 , 0.03], # learning rate\n",
    "        \"subsample\": [0.5, 0.6, 0.7], # prevents overfitting by using % of the testing rows per tree\n",
    "        \"colsample_bytree\": [0.8, 0.7, 0.9] # same as subsampling but hides features (RF feature bagging)\n",
    "     }\n",
    "    },\n",
    "    X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "# LightGBM\n",
    "lgbm_results, lgbm_model_pipe = tune_and_evaluate(\n",
    "    \"LightGBM\", LGBMRegressor(\n",
    "        random_state= 67,\n",
    "        n_jobs = -1,\n",
    "    ),\n",
    "    {\"method\": \"baysian\", # Using baysian search due being more compatible \n",
    "     \"n_iter\": 50, # baysian doesn't need 100\n",
    "     \"params\": {\n",
    "         \"objective\": Categorical([\"regression\"]),\n",
    "         \"boosting_type\": Categorical([\"dart\", \"gbdt\"]), # dart boosing is effective but takes alot of time due to the larger processing load\n",
    "         \"max_depth\": Integer(3, 6),\n",
    "         \"learning_rate\": Real(0.1, 0.3), # eta\n",
    "         \"num_leaves\": Integer(100, 200), \n",
    "         \"n_estimators\": Integer(900, 1500)\n",
    "     }\n",
    "            \n",
    "    },\n",
    "    X_train, X_test, y_train, y_test\n",
    "        )\n",
    "\n",
    "comparison = pd.DataFrame([lr_results, en_results, rfr_results, xgb_results, lgbm_results])\n",
    "comparison.sort_values(\"  R2\", ascending = False).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cea517",
   "metadata": {},
   "source": [
    "### Linearity Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae266",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = lr_model_pipe[:-1].transform(X_test)\n",
    "\n",
    "y_pred = lr_model_pipe.predict(X_test)\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Visual diagnostics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# 1. Linearity\n",
    "axes[0, 0].scatter(y_pred, residuals, alpha=0.5, color=\"grey\", edgecolor=\"black\")\n",
    "axes[0, 0].axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "axes[0, 0].set_xlabel(\"Predicted Values\")\n",
    "axes[0, 0].set_ylabel(\"Residuals\")\n",
    "axes[0, 0].set_title(\"1. Linearity: Residuals vs Predicted\")\n",
    "\n",
    "# 2. Normality of Residuals\n",
    "sm.qqplot(residuals, line=\"45\", ax=axes[0, 1], markerfacecolor=\"grey\", markeredgecolor=\"black\")\n",
    "axes[0, 1].set_title(\"2. Normality: Q-Q Plot of Residuals\")\n",
    "\n",
    "# 3. Homoscedasticity\n",
    "standardized_residuals = (residuals - residuals.mean()) / residuals.std()\n",
    "axes[1, 0].scatter(y_pred, np.sqrt(np.abs(standardized_residuals)), alpha=0.5, color=\"grey\", edgecolor=\"black\")\n",
    "axes[1, 0].set_xlabel(\"Predicted Values\")\n",
    "axes[1, 0].set_ylabel(\"√|Standardized Residuals|\")\n",
    "axes[1, 0].set_title(\"3. Homoscedasticity: Scale-Location Plot\")\n",
    "\n",
    "# 4. Residuals Distribution\n",
    "sns.histplot(residuals, kde=True, color=\"grey\", edgecolor=\"black\", ax=axes[1, 1])\n",
    "axes[1, 1].axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "axes[1, 1].set_title(\"4. Residuals Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical assumptions\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Assumption Test Results\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Shapiro-Wilk Test\n",
    "sample_residuals = residuals.sample(min(500, len(residuals)), random_state=42) if len(residuals) > 500 else residuals\n",
    "shapiro_stat, shapiro_p = stats.shapiro(sample_residuals)\n",
    "print(f\"\\n1. Normality (Shapiro-Wilk Test):\")\n",
    "print(f\"   Statistic: {shapiro_stat:.4f}, p-value: {shapiro_p:.4f}\")\n",
    "print(f\"   Result: {'✓ Normal' if shapiro_p > 0.05 else '✗ Not Normal'} (α=0.05)\")\n",
    "\n",
    "# Durbin-Watson Test\n",
    "# We use .values to ensure we pass a clean array to avoid indexing bugs\n",
    "dw_stat = durbin_watson(residuals.values)\n",
    "print(f\"\\n2. Independence (Durbin-Watson Test):\")\n",
    "print(f\"   Statistic: {dw_stat:.4f}\")\n",
    "print(f\"   Result: {'✓ No autocorrelation' if 1.5 < dw_stat < 2.5 else '✗ Possible autocorrelation'}\")\n",
    "\n",
    "# Breusch-Pagan Test \n",
    "# CRITICAL: Using X_test_transformed here ensures all features are numeric\n",
    "X_test_with_const = sm.add_constant(X_test_transformed)\n",
    "bp_stat, bp_p, _, _ = het_breuschpagan(residuals, X_test_with_const)\n",
    "print(f\"\\n3. Homoscedasticity (Breusch-Pagan Test):\")\n",
    "print(f\"   Statistic: {bp_stat:.4f}, p-value: {bp_p:.4f}\")\n",
    "print(f\"   Result: {'✓ Homoscedastic' if bp_p > 0.05 else '✗ Heteroscedastic'} (α=0.05)\")\n",
    "\n",
    "# Model Performance\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Model Performance Metrics\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c0e5d",
   "metadata": {},
   "source": [
    "### Correlation Matrix (After transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e582699",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engine = xgb_model_pipe[:-1]\n",
    "\n",
    "X_transformed = data_engine.transform(X_train)\n",
    "\n",
    "coltrans = xgb_model_pipe.named_steps[\"encoding_scaling\"]\n",
    "feature_names = list(coltrans.get_feature_names_out()) + [\"Cluster\"]\n",
    "\n",
    "df_corr = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "df_corr[\"Price_euros\"] = y_train.values\n",
    "\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(df_corr.corr(),\n",
    "            annot=False,\n",
    "            cmap=\"Greens\",\n",
    "            linewidths=0.5)\n",
    "plt.title(\"Correlation Matrix (After)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176e392",
   "metadata": {},
   "source": [
    "## Feature Importance\n",
    "Although it would seem that some features are redundant looking at the chart but the numbers say otherwise, keeping these features although they don't deliver significant importance random forest uses feature bagging which uses different combinations which could result in better accuracy overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd071c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the actual model inside the pipeline\n",
    "xgb_core = xgb_model_pipe.named_steps[\"model\"].regressor_\n",
    "coltrans = xgb_model_pipe.named_steps[\"encoding_scaling\"]\n",
    "cluster = xgb_model_pipe.named_steps[\"cluster\"]\n",
    "\n",
    "feature_names = cluster.get_feature_names_out(coltrans.get_feature_names_out())\n",
    "importances = xgb_core.feature_importances_\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by = \"importance\", ascending = False)\n",
    "\n",
    "plt.figure(figsize = (12,10))\n",
    "sns.barplot(x = \"importance\",\n",
    "            y = \"feature\",\n",
    "            data = feature_importance,\n",
    "            palette = \"Set2\",\n",
    "            edgecolor = \"black\")\n",
    "plt.title(\"Feature importance for model\")\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441e80e",
   "metadata": {},
   "source": [
    "## Identifying our K\n",
    "Using **Elbow plot** to identify the best K for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9120be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_cluster_pipe = xgb_model_pipe[: -2] # Setting up pipeline without the y_transformation to for clustering set up\n",
    "\n",
    "X_transformed = pre_cluster_pipe.transform(X_train) # Scaling & encoding data\n",
    "\n",
    "inertia = []\n",
    "\n",
    "for k in range(1, 10):\n",
    "    km = KMeans(n_clusters = k, \n",
    "                random_state = 42,\n",
    "                n_init = 10) # Clusters 10 times to avoid bad clusterings\n",
    "    km.fit_transform(X_transformed)\n",
    "    inertia.append(km.inertia_)\n",
    "    \n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.lineplot(x = range(1, 10),\n",
    "                y = inertia,\n",
    "                marker = \"o\",\n",
    "                color = \"red\")\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Intertia\")\n",
    "plt.title(\"Elbow Plot\")\n",
    "plt.show()\n",
    "print(\"Best K = 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f7d9a5",
   "metadata": {},
   "source": [
    "Now I'll setup the clustering algorithm in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ed44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster.kmeans_.predict(X_transformed)\n",
    "\n",
    "# Attaching Cluster column back to the training dataset\n",
    "cluster_df = X_train.copy()\n",
    "cluster_df[\"Cluster\"] = labels\n",
    "cluster_df[\"Price\"] = y_train.values\n",
    "\n",
    "# Visualizing clusters\n",
    "pca = PCA(n_components = 2, random_state = 42)\n",
    "X_2d = pca.fit_transform(X_transformed)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 8))\n",
    "\n",
    "scatter1 = ax1.scatter(X_2d[:, 0],\n",
    "                       X_2d[:, 1],\n",
    "                       c = labels,\n",
    "                       cmap = \"Set2\",\n",
    "                       alpha = 0.6,\n",
    "                       edgecolor = \"black\",\n",
    "                       linewidth = 0.3)\n",
    "ax1.set_title(\"Laptop Clusters (PCA Projection)\")\n",
    "ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "ax1.legend(*scatter1.legend_elements(), title = \"Cluster\")\n",
    "\n",
    "scatter2 = ax2.scatter(X_2d[:, 0],\n",
    "                       X_2d[:, 1],\n",
    "                       c = y_train.values,\n",
    "                       cmap = \"RdYlGn\",\n",
    "                       alpha = 0.6,\n",
    "                       edgecolor = \"black\",\n",
    "                       linewidth = 0.3)\n",
    "ax2.set_title(\"Price Distribution (PCA Projection)\")\n",
    "ax2.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)\")\n",
    "ax2.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)\")\n",
    "plt.colorbar(scatter2, ax = ax2, label = \"Price (€)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e953feb7",
   "metadata": {},
   "source": [
    "## Business Insights\n",
    "\n",
    "Based on the cluster analysis, laptops naturally group into distinct market segments:\n",
    "\n",
    "- **Cluster 0** — Budget laptops: Low RAM, basic CPUs, smaller screens\n",
    "- **Cluster 1** — Mid-range notebooks: Standard specs, most popular segment  \n",
    "- **Cluster 2** — Premium ultrabooks: High resolution, lightweight, premium pricing\n",
    "- **Cluster 3** — Performance/Gaming: High RAM, powerful CPUs, heaviest & most expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price breakdown per cluster\n",
    "price_summary = cluster_df.groupby(\"Cluster\")[\"Price\"].agg(\n",
    "    [\"mean\", \"median\", \"count\", \"min\", \"max\"]\n",
    ").round(2)\n",
    "price_summary.columns = [\"Mean €\", \"Median €\", \"Count\", \"Min €\", \"Max €\"]\n",
    "print(\"                                                                    === Price per Cluster ===\")\n",
    "display(price_summary)\n",
    "\n",
    "# Hardware profile per cluster\n",
    "hardware_summary = cluster_df.groupby(\"Cluster\").agg(\n",
    "    Avg_Ram = (\"Ram\", \"mean\"),\n",
    "    Avg_Weight = (\"Weight\", \"mean\"),\n",
    "    Avg_Inches = (\"Inches\", \"mean\"),\n",
    "    Avg_CPU_freq = (\"CPU_freq\", \"mean\"),\n",
    "    Avg_Storage = (\"PrimaryStorage\", \"mean\"),\n",
    "    Top_Type = (\"TypeName\", lambda x: x.mode()[0]),\n",
    "    Top_Company = (\"Company\", lambda x: x.mode()[0]),\n",
    "    Top_OS = (\"OS\", lambda x: x.mode()[0])\n",
    ").round(2)\n",
    "print(\"\\n                                                                === Hardware Profile per Cluster ===\")\n",
    "display(hardware_summary)\n",
    "\n",
    "# Cluster size distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16, 6))\n",
    "\n",
    "cluster_df.groupby(\"Cluster\")[\"Price\"].mean().plot(kind = \"bar\",\n",
    "                                                   color = sns.color_palette(\"Set2\"),\n",
    "                                                   edgecolor = \"black\",\n",
    "                                                   ax = ax1)\n",
    "ax1.set_title(\"Average Price per Cluster\")\n",
    "ax1.set_ylabel(\"Price (€)\")\n",
    "ax1.tick_params(axis = \"x\", rotation = 0)\n",
    "\n",
    "cluster_df[\"Cluster\"].value_counts().sort_index().plot(kind = \"pie\",\n",
    "                                                       colors = sns.color_palette(\"Set2\"),\n",
    "                                                       autopct = \"%1.1f%%\",\n",
    "                                                       ax = ax2)\n",
    "ax2.set_title(\"Cluster Size Distribution\")\n",
    "ax2.set_ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1625cf2",
   "metadata": {},
   "source": [
    "# Model deployment\n",
    "Dumping the models into a folder for later use in dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba96e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok = True)\n",
    "\n",
    "models = {\n",
    "    \"linear_regression\": lr_model_pipe,\n",
    "    \"elastic_net\": en_model_pipe,\n",
    "    \"random_forest_regressor\": rfr_model_pipe,\n",
    "    \"xgboost\": xgb_model_pipe,\n",
    "    \"lightgbm\": lgbm_model_pipe\n",
    "}\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    joblib.dump(pipe, f\"models/{name}_pipe.joblib\")\n",
    "    print(f\"✅ Saved {name} → models/{name}_pipe.joblib\")\n",
    "    \n",
    "joblib.dump(comparison, \"models/comparison.joblib\")\n",
    "joblib.dump({\"X_test\": X_test, \"y_test\": y_test}, \"models/test_data.joblib\")\n",
    "\n",
    "print(\"\\nAll models saved to ./models/ successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
